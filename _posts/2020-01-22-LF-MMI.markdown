---
title: "Lattice Free Maximum Mutual Information (LF-MMI)"
layout: post
date: 2020-01-22 10:27
image: /assets/images/markdown.jpg
headerImage: false
tag:
- Machine Learning
star: false
category: blog
author: MatthewWiesner
description: Everything about LF-MMI
---

I'm writing this to remember all of the details of MMI and LF-MMI. I probably missed a few things, but hopefully the main concepts are covered. If you are reading this and you already know what MMI is and you are just looking for the details of LF-MMI, skip to the LF-MMI section.

tldr;

LF-MMI is just like lattice based MMI, but replaces utterance specific denomiator lattices with a globally shared denominator graph constructed by means of a 4-gram phone language model. Some tricks to prevent overfitting are usually required including cross-entropy regularization via multitask learning and L-2 regularization on the network outputs. Since the network is directly trained to produce pseudo-likelihoods, no prior normalization is required.   

__________________________________________________________________________

The LF-MMI objective function is a particular discriminative objective function used especially in hybrid HMM-DNN ASR.
Discriminative objective functions are of interest because they allow us not only to train models to make the correct output sequence
more likely, but they also learn to make incorrect sequences less likely. In other words they are trained to maximize the separation
between the correct and incorrect answers, or to discriminate between correct and incorrect answers rather than simply assign high weights to the correct sequences.

One such objective function is called the **Maximum Mutual Information** (MMI) objective function. It is also sometimes referred to as
**Maximum Conditional Likelihood Estimation**. LF-MMI is essentially just the MMI objective function that has been modified to enable 
training ASR systems on GPU. I describe these modificaitons later. For now, I am just going to describe the MMI objective function.

## Relationship of Maximum Mutual Information Objective to Mutual Information
The MMI objective function is called MMI, because it can be derived from maximizing the mutual information between the input
$$X$$, and output $$W$$ sequences. First the MMI objective function is defined to be

$$F_{MMI} = \sum_{r=1}^N \log{\frac{p_{\theta}\left(X_r | W_r\right)p\left(W_r\right)}{\sum_{W} p_{\theta}\left(X_r | W\right)p\left(W\right)}}$$

In this function, $$r$$ indexes the training examples (utterances or short chunks of audio), $$X_r$$ are the audio features corresponding to chunk $$r$$, and $$W_r$$ is the reference transcript. Probability density functions (PDFs) subscripted by $$\theta$$ are those parameterized with learnable parameters $$\theta$$. This objective function can be shown to be equivalent to maximizing the mutual information over the parameter space between the input and output sequences.

$$ arg\max_\theta I_\theta \left(X_r; W_r\right) = arg\max_\theta H\left(W_r\right) - H_\theta\left(W_r | X_r\right)$$

In general since we are only trying to model the relationship between inputs and outputs, the only parameters we are able to optimize
are those responsible for the conditional distribution $$p_\theta\left(W | X\right)$$. The distribution $$p\left(W\right)$$ is estimated
from the training transcripts and is considered fixed. In the case of ASR, this simply corresponds to a language model.

From this we see that the above optimization problem is equivalent to

$$ arg\max_\theta H\left(W_r\right) - H_\theta\left(W_r | X_r\right) = arg\min_\theta H_{\theta}\left(W_r | X_r\right)$$

Using the definition of conditional entropy we have that

\begin{align}
\mbox{ (1)   } H_{\theta}\left(W_r | X_r\right) &=& E_{p\left(X_r, W_r\right)} \left[ - \log{p_{\theta}\left(W_r | X_r\right)}\right] \\\
\mbox{ (2)   } &=& E_{p\left(X_r, W_r\right)} \left[ - \log{\frac{p_{\theta}\left(X_r | W_r\right) p\left(W_r\right)}{p\left(X_r\right)}}\right] \\\
\mbox{ (3)   } &=& E_{p\left(X_r, W_r\right)} \left[ - \log{\frac{p_{\theta}\left(X_r | W_r\right) p\left(W_r\right)}{\sum_{W} p_{\theta}\left(X_r | W\right) p\left(W\right)}}\right] 
\end{align}

Line (1) from above is the from the definition of conditional entropy. Line (2) uses Bayes rule to factorize the posterior distribution.  Line (4) simply factorizes the joint distribution $$p_{\theta}\left(X, W\right)$$
into a product of the conditional and marginal distributions.

Then, using the law of large numbers we note that ...

$$ E_{p\left(X_r, W_r\right)} \left[ - \log{\frac{p_{\theta}\left(X_r | W_r\right) p\left(W_r\right)}{\sum_{W} p_{\theta}\left(X_r | W\right) p\left(W\right)}}\right]  = \lim_{N \to \infty} \frac{-1}{N} \sum_{r=1}^N\log{\frac{p_{\theta}\left(X_r | W_r\right) p\left(W_r\right)}{\sum_{W} p_{\theta}\left(X_r | W\right) p\left(W\right)}}$$

So finally, by approximating this limit by using a finite sample size $$N$$ ...

$$ E_{p\left(X_r, W_r\right)} \left[ - \log{\frac{p_{\theta}\left(X_r | W_r\right) p\left(W_r\right)}{\sum_{W} p_{\theta}\left(X_r | W\right) p\left(W\right)}}\right] \simeq \frac{-1}{N} \sum_{r=1}^{N}\log{\frac{p_{\theta}\left(X_r | W_r\right) p\left(W_r\right)}{\sum_{W} p_{\theta}\left(X_r | W\right) p\left(W\right)}}$$

And we convert the minimization problem

$$arg\min_{\theta} H_{\theta}\left(W_r | X_r\right) $$

into a maximization problem by negating the conditional entropy and dropping the constant $$N$$ which is just a scaling factor and won't change the optimal parameter values. This leaves us with the originally presented expression for the MMI objective function.

## Gradient of MMI
One way to train the parameters of $$p_{\theta}\left(X_r | W_r\right)$$ is by gradient descent. In practice, rather than working directly with $$p_{\theta}\left(X_r | W_r\right)$$ it is easier to work with $$log{p_{\theta}\left(X_r | W_r\right)}$$.

There is also an identity using gradients of logarithms that will prove useful in this derivation ...

\begin{align}
\nabla_{\theta} \log{x\left(\theta\right\)} & = \frac{1}{x\left(\theta\right)} \cdot \nabla_{\theta} \ x\left(\theta\right) \\\
\implies x\left(\theta\right) \cdot \nabla_{\theta} \log{x\left(\theta\right)} & = \nabla_{\theta} \ x\left(\theta\right)  
\end{align}

Equipped with this identity proceed to take the gradient ...

$$\nabla_{\theta} F_{MMI} = \nabla_{\theta} \sum_{r=1}^N \log{\frac{p_{\theta}\left(X_r | W_r\right)p\left(W_r\right)}{\sum_{W} p_{\theta}\left(X_r | W\right)p\left(W\right)}},$$

$$ = \sum_{r=1}^N \left[\nabla_{\theta} \log{p_{\theta}\left(X_r | W_r \right)} + \nabla_{\theta} \log{p\left(W_r\right)} - \nabla_{\theta} \log{\sum_{W} p_{\theta}\left(X_r | W \right) p\left(W \right)}\right]$$

by linearity of the gradient and splitting up the log of products of distributions into a sum of log distributions. Since $$p\left(W_r\right)$$ is constant with respect to $$\theta$$, this term goes to $$0$$.

$$ \implies \nabla_{\theta} F_{MMI} = \sum_{r=1}^N \left[\nabla_{\theta} \log{p_{\theta}\left(X_r | W_r \right)} - \nabla_{\theta} \log{\sum_{W} p_{\theta}\left(X_r | W \right) p\left(W \right)}\right]$$

$$ = \sum_{r=1}^N \left[\nabla_{\theta} \log{p_{\theta}\left(X_r | W_r \right)} - \frac{\nabla_{\theta}\sum_{W} p_{\theta}\left(X_r | W \right) p\left(W \right)}{\sum_{W} p_{\theta}\left(X_r | W \right) p\left(W \right)}\right]$$

$$ = \sum_{r=1}^N \left[\nabla_{\theta} \log{p_{\theta}\left(X_r | W_r \right)} - \frac{\nabla_{\theta}\sum_{W} p_{\theta}\left(X_r | W \right) p\left(W \right)}{\sum_{W} p_{\theta}\left(X_r | W \right) p\left(W \right)}\right]$$

$$ = \sum_{r=1}^N \left[\nabla_{\theta} \log{p_{\theta}\left(X_r | W_r \right)} - \frac{\sum_{W} p\left(W \right) \nabla_{\theta} p_{\theta}\left(X_r | W \right)}{\sum_{W} p_{\theta}\left(X_r | W \right) p\left(W \right)}\right]$$

Now using our identity regarding gradients of logarithms, we see that 

$$\nabla_{\theta} p_{\theta}\left(X_r | W \right) = p_{\theta}\left(X_r | W \right) \nabla_{\theta} \log{p_{\theta}\left(X_r | W \right)}$$

Making this substituion we have finally ...
$$
\boxed{\nabla_{\theta} F_{MMI} = \sum_{r=1}^N \left[\nabla_{\theta} \log{p_{\theta}\left(X_r | W_r \right)} - \frac{\sum_{W} p\left(W \right) p_{\theta}\left(X_r | W \right) \nabla_{\theta} \log{p_{\theta}\left(X_r | W \right)}}{\sum_{W} p_{\theta}\left(X_r | W \right) p\left(W \right)}\right]}
$$

## Gradient of $$\log{p_{\theta}\left(X_r | W \right)}$$
We now must discuss how we have parameterized $$p_{\theta}\left(X_r | W \right)$$. First, note that 

$$\nabla_{\theta} \log{p_{\theta}\left(X_r | W \right)} = \nabla_{\theta} \log{p_{\theta}\left(X_r | W \right)} + \nabla_{\theta} \log{p\left(W \right)} = \nabla_{\theta} \log{p_{\theta}\left(X_r, W \right)}$$

This means we can consider the gradient of $$\log{p_{\theta}\left(X_r, W \right)}$$
instead of $$\log{p_{\theta}\left(X_r | W \right)}$$. This happens to be very convenient when
$$p_{\theta}\left(X_r, W\right)$$ is parameterized by an HMM.

In general, a particular word sequence can be pronounced in multiple ways. In Hybrid HMM-DNN ASR, we create 3-state HMM models for every possible triphone. In this way a word sequence can be decomposed into a sequences of 3-state triphone HMMs which are strung together to form a model of the word sequence. The collection of these sequences forms a graph which we use to model a particular word sequence. The graph representing all possible word sequences clearly contains more many more paths, but it is still of the same form.

We can now describe $$p_{\theta}\left(X_r, W\right)$$ in terms of the sum of the probabilities of all paths $$\pi_W = \{s_0, \ldots, s_T\}$$ in the graph, where the graph consists of all valid HMM state sequences for the word sequence $$W$$. Again a path is just a sequence of states in the graph. Also note that all the paths must consume the same number of frames, and hence they are all of the same length $$T$$. A frame of speech in the sequence $X_r$ at time $t$ is denoted $x_{rt}$. For the purposes of MMI, we consider the transitions probabilities to be fixed. The only parameters that we are going to learn are the observation probabilities.

$$p_{\theta}\left(X_r, W\right) = \sum_{\pi_W} \prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right)$$

We now take gradients ...

$$\nabla_{\theta} \log{p_{\theta}\left(X_r, W\right)} = \nabla_{\theta} \log{\sum_{\pi_W} \prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right)}$$

$$ = \frac{1}{\sum_{\pi_W} \prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right)} \cdot \nabla_{\theta} \sum_{\pi_W} \prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right) $$

$$ = \frac{1}{\sum_{\pi_W} \prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right)} \cdot \sum_{\pi_W} \nabla_{\theta} \prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right) \tag{1} $$

We will again use our gradient of logarithms identity and convert the logarithm of the product into a sum of logarithms, by using

$$x_W\left(\theta\right) = \prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right)$$.

Since 

$$\nabla_{\theta} \log{\prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right)} = \sum_{t=0}^{T-1} \nabla_{\theta} \log{p_{\theta}\left(x_t | s_t\right)} + \nabla_{\theta} \log{p\left(s_t | s_{t-1}\right)}$$

$$ = \sum_{t=0}^{T-1} \nabla_{\theta} \log{p_{\theta}\left(x_t | s_t\right)}$$

We have that 

$$\nabla_{\theta} \prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right) = \prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right) \cdot \sum_{t=0}^{T-1} \nabla_{\theta} \log{p_{\theta}\left(x_t | s_t\right)}$$

We also note that 

$$ \sum_{\pi_W} x_W\left(\theta\right) \sum_{t=0}^{T-1} \nabla_{\theta} \log{p_{\theta}\left(x_t | s_t\right)} = \sum_{\pi_W} \sum_{t=0}^{T-1} x_W\left(\theta\right) \nabla_{\theta} \log{p_{\theta}\left(x_t | s_t \right)} $$

$$ = \sum_{t=0}^{T-1} \sum_{\pi_W} \nabla_{\theta} \log{p_{\theta}\left(x_t | s_t\right)} x_W\left(\theta\right)$$

Substituting this into (1), we have 

$$ \nabla_{\theta} \log{p_{\theta}\left(X_r, W\right)} = \frac{1}{\sum_{\pi_W} \prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right)} \sum_{t=0}^{T-1} \sum_{\pi_W} \nabla_{\theta} \log{p_{\theta}\left(x_t | s_t\right)} \prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right)$$

Finally pulling the denominator inside the first summation we have ...

$$ \nabla_{\theta} \log{p_{\theta}\left(X_r, W\right)} = \sum_{t=0}^{T-1} \frac{\sum_{\pi_W} \nabla_{\theta} \log{p_{\theta}\left(x_t | s_t\right)} \prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right)}{\sum_{\pi_W} \prod_{t=0}^{T-1} p_{\theta}\left(x_t | s_t\right) p\left(s_t | s_{t-1}\right)}$$




