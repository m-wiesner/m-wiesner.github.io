---
title: "Lattice Free Maximum Mutual Information (LF-MMI)"
layout: post
date: 2020-01-22 10:27
image: /assets/images/markdown.jpg
headerImage: false
tag:
- Machine Learning
star: false
category: blog
author: MatthewWiesner
description: Everything about LF-MMI
---

I'm writing this to remember all of the details of MMI and LF-MMI, especially the gradient computation that I worked through with [Desh Raj's page]. I probably missed a few things, but hopefully the main concepts are covered. If you are reading this and you already know what MMI is and you are just looking for the details of LF-MMI, skip to the LF-MMI section.

tldr;

LF-MMI is just like lattice based MMI, but replaces utterance specific denomiator lattices with a globally shared denominator graph constructed by means of a 4-gram phone language model. Some tricks to prevent overfitting are usually required including cross-entropy regularization via multitask learning and L-2 regularization on the network outputs. Since the network is directly trained to produce pseudo-likelihoods, no prior normalization is required.   

__________________________________________________________________________

The LF-MMI objective function is a particular discriminative objective function used especially in hybrid HMM-DNN ASR.
Discriminative objective functions are of interest because they allow us not only to train models to make the correct output sequence
more likely, but they also learn to make incorrect sequences less likely. In other words they are trained to maximize the separation
between the correct and incorrect answers, or to discriminate between correct and incorrect answers rather than simply assign high weights to the correct sequences.

One such objective function is called the **Maximum Mutual Information** (MMI) objective function. It is also sometimes referred to as
**Maximum Conditional Likelihood Estimation**. LF-MMI is essentially just the MMI objective function that has been modified to enable 
training ASR systems on GPU. I describe these modificaitons later. For now, I am just going to describe the MMI objective function.

## Relationship of Maximum Mutual Information Objective to Mutual Information
The MMI objective function is called MMI, because it can be derived from maximizing the mutual information between the input
$$X$$, and output $$W$$ sequences. First the MMI objective function is defined to be

$$F_{MMI} = \sum_{r=1}^N \log{\frac{p_{\theta}\left(X_r | W_r\right)p\left(W_r\right)}{\sum_{W} p_{\theta}\left(X_r | W\right)p\left(W\right)}}$$

In this function, $$r$$ indexes the training examples (utterances or short chunks of audio), $$X_r$$ are the audio features corresponding to chunk $$r$$, and $$W_r$$ is the reference transcript. Probability density functions (PDFs) subscripted by $$\theta$$ are those parameterized with learnable parameters $$\theta$$. This objective function can be shown to be equivalent to maximizing the mutual information over the parameter space between the input and output sequences.

$$ arg\max_\theta I_\theta \left(X_r; W_r\right) = arg\max_\theta H\left(W_r\right) - H_\theta\left(W_r | X_r\right)$$

In general since we are only trying to model the relationship between inputs and outputs, the only parameters we are able to optimize
are those responsible for the conditional distribution $$p_\theta\left(W | X\right)$$. The distribution $$p\left(W\right)$$ is estimated
from the training transcripts and is considered fixed. In the case of ASR, this simply corresponds to a language model.

From this we see that the above optimization problem is equivalent to

$$ arg\max_\theta H\left(W_r\right) - H_\theta\left(W_r | X_r\right) = arg\min_\theta H_{\theta}\left(W_r | X_r\right)$$

Using the definition of conditional entropy we have that

\begin{align}
\mbox{ (1)   } H_{\theta}\left(W_r | X_r\right) &=& E_{p\left(X_r, W_r\right)} \left[ - \log{p_{\theta}\left(W_r | X_r\right)}\right] \\\
\mbox{ (2)   } &=& E_{p\left(X_r, W_r\right)} \left[ - \log{\frac{p_{\theta}\left(X_r | W_r\right) p\left(W_r\right)}{p\left(X_r\right)}}\right] \\\
\mbox{ (3)   } &=& E_{p\left(X_r, W_r\right)} \left[ - \log{\frac{p_{\theta}\left(X_r | W_r\right) p\left(W_r\right)}{\sum_{W} p_{\theta}\left(X_r | W\right) p\left(W\right)}}\right] 
\end{align}

Line (1) from above is the from the definition of conditional entropy. Line (2) uses Bayes rule to factorize the posterior distribution.  Line (4) simply factorizes the joint distribution $$p_{\theta}\left(X, W\right)$$
into a product of the conditional and marginal distributions.

Then, using the law of large numbers we note that ...

$$ E_{p\left(X_r, W_r\right)} \left[ - \log{\frac{p_{\theta}\left(X_r | W_r\right) p\left(W_r\right)}{\sum_{W} p_{\theta}\left(X_r | W\right) p\left(W\right)}}\right]  = \lim_{N \to \infty} \frac{-1}{N} \sum_{r=1}^N\log{\frac{p_{\theta}\left(X_r | W_r\right) p\left(W_r\right)}{\sum_{W} p_{\theta}\left(X_r | W\right) p\left(W\right)}}$$

So finally, by approximating this limit by using a finite sample size $$N$$ ...

$$ E_{p\left(X_r, W_r\right)} \left[ - \log{\frac{p_{\theta}\left(X_r | W_r\right) p\left(W_r\right)}{\sum_{W} p_{\theta}\left(X_r | W\right) p\left(W\right)}}\right] \simeq \frac{-1}{N} \sum_{r=1}^{N}\log{\frac{p_{\theta}\left(X_r | W_r\right) p\left(W_r\right)}{\sum_{W} p_{\theta}\left(X_r | W\right) p\left(W\right)}}$$

And we convert the minimization problem

$$arg\min_{\theta} H_{\theta}\left(W_r | X_r\right) $$

into a maximization problem by negating the conditional entropy and dropping the constant $$N$$ which is just a scaling factor and won't change the optimal parameter values. This leaves us with the originally presented expression for the MMI objective function.

## Acoustic Modeling with HMMs Background
The HMM acoustic model is constructed as follows.
* Words are modeled as a sequence of units. In traditional ASR these units are triphones, but even just a sequence of letters would probably work fine. A single word could corresponding to different allowable sequences of units.

>EITHER --> IY - TH - ER

>EITHER --> AY - TH - ER

* For each of these units, there is an associated HMM. Traditionally this is a 3-state HMM, but for reasons I'll explain later, we tend to use a 1-state HMM instead. It would look something like this ...

![](/LF-MMI/graphviz.png){:height="50%" width="50%"}

* In an HMM we model $$p_{\theta}\left(X_r | W_r\right)$$ where $$X_r = \left{x_0, \ldots, x_{T-1} \right}$$ is a length $$T$$ sequence as ...

$$ p_{\theta}\left(X_r | W_r \right) = \sum_{\pi_r} \prod_{t=0}^{T-1} p_{\theta}\left(x_t | \pi_r^t\right) p\left(\pi_r^t | \pi_r^{t-1} \right)$$

$$\pi_r$$ corresponds to one of the valid paths through the HMM for the word sequence $$w_r$$. $$\pi_r^t$$ is the state at time $$t$$ along the path $$\pi_r$$. 

## Gradient of MMI
I am going to assume that the underlying acoustic model is an HMM. I am also going to assume that using a Hybrid HMM-DNN model, whereby the log emission probabilities for the $$D$$ states $$s_1, \ldots, s_D$$ in our HMM are modeled by the DNN output activations. We define $$y_s^t$$ to be the DNN activations at time $$t$$ for a state $$s$$. In other terms $$y_s^t = \log{p_\{theta}\left(x_t | s \right)$$

Plugging this into the expression for $$F_{MMI}$$ we get

$$F_{MMI} = \sum_{r} \log{\frac{\pi}{}}$$ 


